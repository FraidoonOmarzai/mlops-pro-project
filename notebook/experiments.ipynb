{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37bebe2a",
   "metadata": {},
   "source": [
    "<h1 align=center>Data Ingestion</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40dd410a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\44787\\\\Desktop\\\\mlops-pro-project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d623e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "from src.logger import logger\n",
    "from src.exception import CustomException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448f10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    \"\"\"Configuration for data ingestion component.\"\"\"\n",
    "    raw_data_path: str\n",
    "    train_data_path: str\n",
    "    test_data_path: str\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d534799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    \"\"\"\n",
    "    Handles data loading and splitting into train/test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        \"\"\"\n",
    "        Initialize DataIngestion component.\n",
    "        \n",
    "        Args:\n",
    "            config: DataIngestionConfig object with paths and parameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        logger.info(\"Data Ingestion component initialized\")\n",
    "    \n",
    "    def initiate_data_ingestion(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Load data and split into train/test sets.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (train_data_path, test_data_path)\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting data ingestion process\")\n",
    "        \n",
    "        try:\n",
    "            # Read the dataset\n",
    "            logger.info(f\"Reading dataset from {self.config.raw_data_path}\")\n",
    "            df = pd.read_csv(self.config.raw_data_path)\n",
    "            logger.info(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "            \n",
    "            # Basic info logging\n",
    "            logger.info(f\"Columns: {list(df.columns)}\")\n",
    "            logger.info(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "            logger.info(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "            \n",
    "            # Create directory for processed data\n",
    "            os.makedirs(os.path.dirname(self.config.train_data_path), exist_ok=True)\n",
    "            \n",
    "            # Split the data\n",
    "            logger.info(f\"Splitting data with test_size={self.config.test_size}\")\n",
    "            train_set, test_set = train_test_split(\n",
    "                df,\n",
    "                test_size=self.config.test_size,\n",
    "                random_state=self.config.random_state,\n",
    "                stratify=df.iloc[:, -1] if 'Churn' in df.columns else None  # Stratify on target\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Train set shape: {train_set.shape}\")\n",
    "            logger.info(f\"Test set shape: {test_set.shape}\")\n",
    "            \n",
    "            # Save train and test sets\n",
    "            train_set.to_csv(self.config.train_data_path, index=False, header=True)\n",
    "            test_set.to_csv(self.config.test_data_path, index=False, header=True)\n",
    "            \n",
    "            logger.info(\"Data ingestion completed successfully\")\n",
    "            logger.info(f\"Train data saved to: {self.config.train_data_path}\")\n",
    "            logger.info(f\"Test data saved to: {self.config.test_data_path}\")\n",
    "            \n",
    "            return (\n",
    "                self.config.train_data_path,\n",
    "                self.config.test_data_path\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"Error in data ingestion\")\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def get_data_info(self) -> dict:\n",
    "        \"\"\"\n",
    "        Get information about the ingested data.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with data statistics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.config.raw_data_path)\n",
    "            \n",
    "            info = {\n",
    "                'total_rows': len(df),\n",
    "                'total_columns': len(df.columns),\n",
    "                'columns': list(df.columns),\n",
    "                'missing_values': df.isnull().sum().to_dict(),\n",
    "                'duplicates': int(df.duplicated().sum()),\n",
    "                'dtypes': df.dtypes.astype(str).to_dict()\n",
    "            }\n",
    "            \n",
    "            return info\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd11d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_ingestion_config(config_dict: dict) -> DataIngestionConfig:\n",
    "    \"\"\"\n",
    "    Create DataIngestionConfig from dictionary.\n",
    "    \n",
    "    Args:\n",
    "        config_dict: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        DataIngestionConfig object\n",
    "    \"\"\"\n",
    "    return DataIngestionConfig(\n",
    "        raw_data_path=config_dict.data_ingeti.raw_data_path,\n",
    "        train_data_path=config_dict.train_data_path,\n",
    "        test_data_path=config_dict.test_data_path,\n",
    "        test_size=config_dict.test_size,\n",
    "        random_state=config_dict.random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-06 18:25:30,342] INFO - ChurnPrediction - yaml file: configs\\config.yaml loaded successfully\n",
      "[2025-11-06 18:25:30,356] INFO - ChurnPrediction - Data Ingestion component initialized\n",
      "[2025-11-06 18:25:30,357] INFO - ChurnPrediction - Starting data ingestion process\n",
      "[2025-11-06 18:25:30,358] INFO - ChurnPrediction - Reading dataset from data/raw/churn_data.csv\n",
      "[2025-11-06 18:25:30,426] INFO - ChurnPrediction - Dataset loaded successfully. Shape: (7043, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-06 18:25:30,429] INFO - ChurnPrediction - Columns: ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
      "[2025-11-06 18:25:30,442] INFO - ChurnPrediction - Missing values: 0\n",
      "[2025-11-06 18:25:30,476] INFO - ChurnPrediction - Duplicates: 0\n",
      "[2025-11-06 18:25:30,479] INFO - ChurnPrediction - Splitting data with test_size=0.2\n",
      "[2025-11-06 18:25:30,504] INFO - ChurnPrediction - Train set shape: (5634, 21)\n",
      "[2025-11-06 18:25:30,506] INFO - ChurnPrediction - Test set shape: (1409, 21)\n",
      "[2025-11-06 18:25:30,585] INFO - ChurnPrediction - Data ingestion completed successfully\n",
      "[2025-11-06 18:25:30,588] INFO - ChurnPrediction - Train data saved to: data/processed/train.csv\n",
      "[2025-11-06 18:25:30,589] INFO - ChurnPrediction - Test data saved to: data/processed/test.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from yaml import safe_load\n",
    "from src.utils.common import read_yaml\n",
    "\n",
    "try:\n",
    "    config_dict = read_yaml(Path(\"configs/config.yaml\"))\n",
    "    config = create_data_ingestion_config(config_dict.data_ingestion)\n",
    "    data_ingestion = DataIngestion(config)\n",
    "    train_data_path, test_data_path = data_ingestion.initiate_data_ingestion()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Data ingestion failed: {e}\")\n",
    "    raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529f844",
   "metadata": {},
   "source": [
    "<h1 align=center>Data Validation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd40cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "# from src.utils.common import save_json\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_json(path, data):\n",
    "    path = Path(path)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b44a1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataValidationConfig:\n",
    "    \"\"\"Configuration for data validation component.\"\"\"\n",
    "    report_path: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2e0f837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "3  7795-CFOCW    Male              0      No         No      45           No   \n",
       "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
       "0  No phone service             DSL             No  ...               No   \n",
       "1                No             DSL            Yes  ...              Yes   \n",
       "2                No             DSL            Yes  ...               No   \n",
       "3  No phone service             DSL            Yes  ...              Yes   \n",
       "4                No     Fiber optic             No  ...               No   \n",
       "\n",
       "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
       "0          No          No              No  Month-to-month              Yes   \n",
       "1          No          No              No        One year               No   \n",
       "2          No          No              No  Month-to-month              Yes   \n",
       "3         Yes          No              No        One year               No   \n",
       "4          No          No              No  Month-to-month              Yes   \n",
       "\n",
       "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
       "0           Electronic check          29.85         29.85    No  \n",
       "1               Mailed check          56.95        1889.5    No  \n",
       "2               Mailed check          53.85        108.15   Yes  \n",
       "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
       "4           Electronic check          70.70        151.65   Yes  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(Path(\"data/raw/churn_data.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c17fc62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   customerID        7043 non-null   object \n",
      " 1   gender            7043 non-null   object \n",
      " 2   SeniorCitizen     7043 non-null   int64  \n",
      " 3   Partner           7043 non-null   object \n",
      " 4   Dependents        7043 non-null   object \n",
      " 5   tenure            7043 non-null   int64  \n",
      " 6   PhoneService      7043 non-null   object \n",
      " 7   MultipleLines     7043 non-null   object \n",
      " 8   InternetService   7043 non-null   object \n",
      " 9   OnlineSecurity    7043 non-null   object \n",
      " 10  OnlineBackup      7043 non-null   object \n",
      " 11  DeviceProtection  7043 non-null   object \n",
      " 12  TechSupport       7043 non-null   object \n",
      " 13  StreamingTV       7043 non-null   object \n",
      " 14  StreamingMovies   7043 non-null   object \n",
      " 15  Contract          7043 non-null   object \n",
      " 16  PaperlessBilling  7043 non-null   object \n",
      " 17  PaymentMethod     7043 non-null   object \n",
      " 18  MonthlyCharges    7043 non-null   float64\n",
      " 19  TotalCharges      7043 non-null   object \n",
      " 20  Churn             7043 non-null   object \n",
      "dtypes: float64(1), int64(2), object(18)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67caaca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation:\n",
    "    \"\"\"\n",
    "    Validates data quality and schema compliance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Expected schema for churn dataset\n",
    "    EXPECTED_COLUMNS = [\n",
    "        'customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "        'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n",
    "        'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
    "        'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "        'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn'\n",
    "    ]\n",
    "    \n",
    "    ## type of TotalCharges is object\n",
    "    # NUMERICAL_COLUMNS = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "    NUMERICAL_COLUMNS = ['tenure', 'MonthlyCharges']\n",
    "    \n",
    "    CATEGORICAL_COLUMNS = [\n",
    "        'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',\n",
    "        'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "        'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "        'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
    "    ]\n",
    "    TARGET_COLUMN = 'Churn'\n",
    "    \n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        \"\"\"\n",
    "        Initialize DataValidation component.\n",
    "        \n",
    "        Args:\n",
    "            config: DataValidationConfig object\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        logger.info(\"Data Validation component initialized\")\n",
    "    \n",
    "    def validate_schema(self, df: pd.DataFrame) -> Dict[str, bool]:\n",
    "        \"\"\"\n",
    "        Validate if dataframe matches expected schema.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with validation results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            validation_results = {}\n",
    "            \n",
    "            # Check if all expected columns are present\n",
    "            missing_columns = set(self.EXPECTED_COLUMNS) - set(df.columns)\n",
    "            validation_results['all_columns_present'] = len(missing_columns) == 0\n",
    "            validation_results['missing_columns'] = list(missing_columns)\n",
    "            \n",
    "            # Check for extra columns\n",
    "            extra_columns = set(df.columns) - set(self.EXPECTED_COLUMNS)\n",
    "            validation_results['extra_columns'] = list(extra_columns)\n",
    "            \n",
    "            # Check data types for numerical columns\n",
    "            numerical_dtype_check = {}\n",
    "            for col in self.NUMERICAL_COLUMNS:\n",
    "                if col in df.columns:\n",
    "                    numerical_dtype_check[col] = pd.api.types.is_numeric_dtype(df[col])\n",
    "            validation_results['numerical_dtypes_correct'] = all(numerical_dtype_check.values())\n",
    "            validation_results['numerical_dtype_details'] = numerical_dtype_check\n",
    "            \n",
    "            logger.info(f\"Schema validation completed: {validation_results}\")\n",
    "            return validation_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def validate_data_quality(self, df: pd.DataFrame) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Validate data quality checks.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with quality check results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            quality_report = {}\n",
    "            \n",
    "            # Check for missing values\n",
    "            missing_values = df.isnull().sum()\n",
    "            quality_report['missing_values'] = missing_values[missing_values > 0].to_dict()\n",
    "            quality_report['total_missing'] = int(df.isnull().sum().sum())\n",
    "            quality_report['missing_percentage'] = round(\n",
    "                (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100, 2\n",
    "            )\n",
    "            \n",
    "            # Check for duplicates\n",
    "            quality_report['duplicate_rows'] = int(df.duplicated().sum())\n",
    "            quality_report['duplicate_percentage'] = round(\n",
    "                (df.duplicated().sum() / len(df)) * 100, 2\n",
    "            )\n",
    "            \n",
    "            # Check for data ranges (numerical columns)\n",
    "            numerical_stats = {}\n",
    "            for col in self.NUMERICAL_COLUMNS:\n",
    "                if col in df.columns:\n",
    "                    numerical_stats[col] = {\n",
    "                        'min': float(df[col].min()),\n",
    "                        'max': float(df[col].max()),\n",
    "                        'mean': float(df[col].mean()),\n",
    "                        'std': float(df[col].std()),\n",
    "                        'negative_values': int((df[col] < 0).sum())\n",
    "                    }\n",
    "            quality_report['numerical_statistics'] = numerical_stats\n",
    "            \n",
    "            # Check target distribution\n",
    "            if self.TARGET_COLUMN in df.columns:\n",
    "                target_dist = df[self.TARGET_COLUMN].value_counts()\n",
    "                quality_report['target_distribution'] = target_dist.to_dict()\n",
    "                quality_report['target_balance_ratio'] = round(\n",
    "                    target_dist.min() / target_dist.max(), 2\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Data quality validation completed\")\n",
    "            return quality_report\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def initiate_data_validation(self, train_path: str, test_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Perform complete data validation on train and test sets.\n",
    "        \n",
    "        Args:\n",
    "            train_path: Path to training data\n",
    "            test_path: Path to test data\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if data passed validation\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting data validation process\")\n",
    "        \n",
    "        try:\n",
    "            # Load data\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "            \n",
    "            logger.info(f\"Loaded train data: {train_df.shape}\")\n",
    "            logger.info(f\"Loaded test data: {test_df.shape}\")\n",
    "            \n",
    "            # Validate schema\n",
    "            train_schema = self.validate_schema(train_df)\n",
    "            test_schema = self.validate_schema(test_df)\n",
    "            \n",
    "            # Validate quality\n",
    "            train_quality = self.validate_data_quality(train_df)\n",
    "            test_quality = self.validate_data_quality(test_df)\n",
    "            \n",
    "            # Compile validation report\n",
    "            validation_report = {\n",
    "                'train_data': {\n",
    "                    'shape': train_df.shape,\n",
    "                    'schema_validation': train_schema,\n",
    "                    'quality_validation': train_quality\n",
    "                },\n",
    "                'test_data': {\n",
    "                    'shape': test_df.shape,\n",
    "                    'schema_validation': test_schema,\n",
    "                    'quality_validation': test_quality\n",
    "                },\n",
    "                'validation_passed': (\n",
    "                    train_schema['all_columns_present'] and \n",
    "                    test_schema['all_columns_present']\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            # Save validation report\n",
    "            save_json(self.config.report_path, validation_report)\n",
    "            logger.info(f\"Validation report saved to: {self.config.report_path}\")\n",
    "            \n",
    "            # Log critical issues\n",
    "            if not validation_report['validation_passed']:\n",
    "                logger.warning(\"Data validation failed! Check the validation report.\")\n",
    "            else:\n",
    "                logger.info(\"Data validation passed successfully!\")\n",
    "            \n",
    "            return validation_report['validation_passed']\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"Error in data validation\")\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c3a780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_validation_config(config_dict: dict) -> DataValidationConfig:\n",
    "    \"\"\"\n",
    "    Create DataValidationConfig from dictionary.\n",
    "    \n",
    "    Args:\n",
    "        config_dict: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        DataValidationConfig object\n",
    "    \"\"\"\n",
    "    return DataValidationConfig(\n",
    "        report_path=config_dict.report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09ea3583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-06 18:39:38,973] INFO - ChurnPrediction - yaml file: configs\\config.yaml loaded successfully\n",
      "[2025-11-06 18:39:38,975] INFO - ChurnPrediction - Data Validation component initialized\n",
      "[2025-11-06 18:39:38,977] INFO - ChurnPrediction - Starting data validation process\n",
      "[2025-11-06 18:39:39,016] INFO - ChurnPrediction - Loaded train data: (5634, 21)\n",
      "[2025-11-06 18:39:39,018] INFO - ChurnPrediction - Loaded test data: (1409, 21)\n",
      "[2025-11-06 18:39:39,020] INFO - ChurnPrediction - Schema validation completed: {'all_columns_present': True, 'missing_columns': [], 'extra_columns': [], 'numerical_dtypes_correct': True, 'numerical_dtype_details': {'tenure': True, 'MonthlyCharges': True}}\n",
      "[2025-11-06 18:39:39,020] INFO - ChurnPrediction - Schema validation completed: {'all_columns_present': True, 'missing_columns': [], 'extra_columns': [], 'numerical_dtypes_correct': True, 'numerical_dtype_details': {'tenure': True, 'MonthlyCharges': True}}\n",
      "[2025-11-06 18:39:39,069] INFO - ChurnPrediction - Data quality validation completed\n",
      "[2025-11-06 18:39:39,088] INFO - ChurnPrediction - Data quality validation completed\n",
      "[2025-11-06 18:39:39,092] INFO - ChurnPrediction - Validation report saved to: artifacts/validation_report.json\n",
      "[2025-11-06 18:39:39,094] INFO - ChurnPrediction - Data validation passed successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_dict = read_yaml(Path(\"configs/config.yaml\"))\n",
    "    data_validation_config = create_data_validation_config(config_dict.data_validation)\n",
    "    data_validation = DataValidation(data_validation_config)\n",
    "    validation_passed = data_validation.initiate_data_validation(\n",
    "        config_dict.data_ingestion.train_data_path,\n",
    "        config_dict.data_ingestion.test_data_path\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Data validation failed: {e}\")\n",
    "    raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faabdd1a",
   "metadata": {},
   "source": [
    "<h1 align=center>Data Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e16f0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "# from src.utils.common import save_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "736ba191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_object(file_path: str, obj):\n",
    "    \"\"\"\n",
    "    Save any Python object to a file using pickle.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file where the object should be saved.\n",
    "        obj: Python object to pickle.\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    if dir_path != \"\":\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Save object\n",
    "    with open(file_path, 'wb') as file_obj:\n",
    "        pickle.dump(obj, file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d549f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataPreprocessingConfig:\n",
    "    \"\"\"Configuration for data preprocessing component.\"\"\"\n",
    "    preprocessor_path: str\n",
    "    numerical_features: list\n",
    "    categorical_features: list\n",
    "    target_column: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec46a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    \"\"\"\n",
    "    Handles feature engineering and data transformation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataPreprocessingConfig):\n",
    "        \"\"\"\n",
    "        Initialize DataPreprocessing component.\n",
    "        \n",
    "        Args:\n",
    "            config: DataPreprocessingConfig object\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.preprocessor = None\n",
    "        logger.info(\"Data Preprocessing component initialized\")\n",
    "    \n",
    "    def get_preprocessor(self) -> ColumnTransformer:\n",
    "        \"\"\"\n",
    "        Create preprocessing pipeline for numerical and categorical features.\n",
    "        \n",
    "        Returns:\n",
    "            ColumnTransformer object with preprocessing pipelines\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Numerical pipeline\n",
    "            numerical_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='median')),\n",
    "                    ('scaler', StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Categorical pipeline\n",
    "            categorical_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Combine pipelines\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numerical_pipeline, self.config.numerical_features),\n",
    "                    ('cat', categorical_pipeline, self.config.categorical_features)\n",
    "                ],\n",
    "                remainder='drop'\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Preprocessing pipeline created successfully\")\n",
    "            logger.info(f\"Numerical features: {self.config.numerical_features}\")\n",
    "            logger.info(f\"Categorical features: {self.config.categorical_features}\")\n",
    "            \n",
    "            return preprocessor\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform data cleaning operations.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting data cleaning\")\n",
    "            df_clean = df.copy()\n",
    "            \n",
    "            # Remove customerID column (not useful for modeling)\n",
    "            if 'customerID' in df_clean.columns:\n",
    "                df_clean = df_clean.drop('customerID', axis=1)\n",
    "                logger.info(\"Dropped customerID column\")\n",
    "            \n",
    "            # Handle TotalCharges - convert to numeric\n",
    "            if 'TotalCharges' in df_clean.columns:\n",
    "                df_clean['TotalCharges'] = pd.to_numeric(\n",
    "                    df_clean['TotalCharges'], \n",
    "                    errors='coerce'\n",
    "                )\n",
    "                logger.info(\"Converted TotalCharges to numeric\")\n",
    "            \n",
    "            # Convert SeniorCitizen to object for categorical encoding\n",
    "            if 'SeniorCitizen' in df_clean.columns:\n",
    "                df_clean['SeniorCitizen'] = df_clean['SeniorCitizen'].astype(str)\n",
    "            \n",
    "            # Remove duplicates\n",
    "            initial_rows = len(df_clean)\n",
    "            df_clean = df_clean.drop_duplicates()\n",
    "            removed_duplicates = initial_rows - len(df_clean)\n",
    "            if removed_duplicates > 0:\n",
    "                logger.info(f\"Removed {removed_duplicates} duplicate rows\")\n",
    "            \n",
    "            logger.info(f\"Data cleaning completed. Final shape: {df_clean.shape}\")\n",
    "            return df_clean\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def initiate_data_preprocessing(self, train_path: str, test_path: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Perform complete data preprocessing.\n",
    "        \n",
    "        Args:\n",
    "            train_path: Path to training data\n",
    "            test_path: Path to test data\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (train_features, test_features, train_target, test_target, preprocessor_path)\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting data preprocessing process\")\n",
    "        \n",
    "        try:\n",
    "            # Load data\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "            \n",
    "            logger.info(f\"Loaded train data: {train_df.shape}\")\n",
    "            logger.info(f\"Loaded test data: {test_df.shape}\")\n",
    "            \n",
    "            # Clean data\n",
    "            train_df = self.clean_data(train_df)\n",
    "            test_df = self.clean_data(test_df)\n",
    "            \n",
    "            # Separate features and target\n",
    "            target_column = self.config.target_column\n",
    "            \n",
    "            X_train = train_df.drop(columns=[target_column])\n",
    "            y_train = train_df[target_column]\n",
    "            \n",
    "            X_test = test_df.drop(columns=[target_column])\n",
    "            y_test = test_df[target_column]\n",
    "            \n",
    "            logger.info(f\"Separated features and target\")\n",
    "            logger.info(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "            logger.info(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "            \n",
    "            # Encode target variable\n",
    "            label_encoder = LabelEncoder()\n",
    "            y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "            y_test_encoded = label_encoder.transform(y_test)\n",
    "            \n",
    "            logger.info(f\"Target encoding: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "            \n",
    "            # Get preprocessing pipeline\n",
    "            self.preprocessor = self.get_preprocessor()\n",
    "            \n",
    "            # Fit and transform training data\n",
    "            logger.info(\"Fitting preprocessor on training data\")\n",
    "            X_train_transformed = self.preprocessor.fit_transform(X_train)\n",
    "            \n",
    "            # Transform test data\n",
    "            logger.info(\"Transforming test data\")\n",
    "            X_test_transformed = self.preprocessor.transform(X_test)\n",
    "            \n",
    "            logger.info(f\"Transformed X_train shape: {X_train_transformed.shape}\")\n",
    "            logger.info(f\"Transformed X_test shape: {X_test_transformed.shape}\")\n",
    "            \n",
    "            # Save preprocessor\n",
    "            save_object(self.config.preprocessor_path, self.preprocessor)\n",
    "            logger.info(f\"Preprocessor saved to: {self.config.preprocessor_path}\")\n",
    "            \n",
    "            # Also save label encoder\n",
    "            label_encoder_path = self.config.preprocessor_path.replace('.pkl', '_label_encoder.pkl')\n",
    "            save_object(label_encoder_path, label_encoder)\n",
    "            logger.info(f\"Label encoder saved to: {label_encoder_path}\")\n",
    "            \n",
    "            logger.info(\"Data preprocessing completed successfully\")\n",
    "            \n",
    "            return (\n",
    "                X_train_transformed,\n",
    "                X_test_transformed,\n",
    "                y_train_encoded,\n",
    "                y_test_encoded,\n",
    "                self.config.preprocessor_path\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"Error in data preprocessing\")\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64ba9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_data_preprocessing_config(config_dict: dict) -> DataPreprocessingConfig:\n",
    "    \"\"\"\n",
    "    Create DataPreprocessingConfig from dictionary.\n",
    "    \n",
    "    Args:\n",
    "        config_dict: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        DataPreprocessingConfig object\n",
    "    \"\"\"\n",
    "    return DataPreprocessingConfig(\n",
    "        preprocessor_path=config_dict.preprocessor_path,\n",
    "        numerical_features=config_dict.numerical_features,\n",
    "        categorical_features=config_dict.categorical_features,\n",
    "        target_column=config_dict.target_column\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "737d6243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-07 17:24:47,967] INFO - ChurnPrediction - yaml file: configs\\config.yaml loaded successfully\n",
      "[2025-11-07 17:24:47,979] INFO - ChurnPrediction - Data Preprocessing component initialized\n",
      "[2025-11-07 17:24:47,980] INFO - ChurnPrediction - Starting data preprocessing process\n",
      "[2025-11-07 17:24:48,097] INFO - ChurnPrediction - Loaded train data: (5634, 21)\n",
      "[2025-11-07 17:24:48,097] INFO - ChurnPrediction - Loaded test data: (1409, 21)\n",
      "[2025-11-07 17:24:48,098] INFO - ChurnPrediction - Starting data cleaning\n",
      "[2025-11-07 17:24:48,119] INFO - ChurnPrediction - Dropped customerID column\n",
      "[2025-11-07 17:24:48,126] INFO - ChurnPrediction - Converted TotalCharges to numeric\n",
      "[2025-11-07 17:24:48,149] INFO - ChurnPrediction - Removed 15 duplicate rows\n",
      "[2025-11-07 17:24:48,150] INFO - ChurnPrediction - Data cleaning completed. Final shape: (5619, 20)\n",
      "[2025-11-07 17:24:48,151] INFO - ChurnPrediction - Starting data cleaning\n",
      "[2025-11-07 17:24:48,154] INFO - ChurnPrediction - Dropped customerID column\n",
      "[2025-11-07 17:24:48,155] INFO - ChurnPrediction - Converted TotalCharges to numeric\n",
      "[2025-11-07 17:24:48,161] INFO - ChurnPrediction - Data cleaning completed. Final shape: (1409, 20)\n",
      "[2025-11-07 17:24:48,165] INFO - ChurnPrediction - Separated features and target\n",
      "[2025-11-07 17:24:48,166] INFO - ChurnPrediction - X_train shape: (5619, 19), y_train shape: (5619,)\n",
      "[2025-11-07 17:24:48,168] INFO - ChurnPrediction - X_test shape: (1409, 19), y_test shape: (1409,)\n",
      "[2025-11-07 17:24:48,179] INFO - ChurnPrediction - Target encoding: {'No': 0, 'Yes': 1}\n",
      "[2025-11-07 17:24:48,181] INFO - ChurnPrediction - Preprocessing pipeline created successfully\n",
      "[2025-11-07 17:24:48,182] INFO - ChurnPrediction - Numerical features: ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
      "[2025-11-07 17:24:48,183] INFO - ChurnPrediction - Categorical features: ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "[2025-11-07 17:24:48,184] INFO - ChurnPrediction - Fitting preprocessor on training data\n",
      "[2025-11-07 17:24:48,275] INFO - ChurnPrediction - Transforming test data\n",
      "[2025-11-07 17:24:48,289] INFO - ChurnPrediction - Transformed X_train shape: (5619, 30)\n",
      "[2025-11-07 17:24:48,290] INFO - ChurnPrediction - Transformed X_test shape: (1409, 30)\n",
      "[2025-11-07 17:24:48,294] INFO - ChurnPrediction - Preprocessor saved to: artifacts/preprocessors/preprocessor.pkl\n",
      "[2025-11-07 17:24:48,298] INFO - ChurnPrediction - Label encoder saved to: artifacts/preprocessors/preprocessor_label_encoder.pkl\n",
      "[2025-11-07 17:24:48,299] INFO - ChurnPrediction - Data preprocessing completed successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_dict = read_yaml(Path(\"configs/config.yaml\"))\n",
    "    data_preprocessing_config = create_data_preprocessing_config(config_dict.data_preprocessing)\n",
    "    data_preprocessing = DataPreprocessing(data_preprocessing_config)\n",
    "    X_train, X_test, y_train, y_test, preprocessor_path = data_preprocessing.initiate_data_preprocessing(\n",
    "        config_dict.data_ingestion.train_data_path,\n",
    "        config_dict.data_ingestion.test_data_path\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Data preprocessing failed: {e}\")\n",
    "    raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6bf83f",
   "metadata": {},
   "source": [
    "<h1 align=center>Model Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c48c3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "# from src.utils.common import save_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a596c615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\44787\\\\Desktop\\\\mlops-pro-project'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5888f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelTrainerConfig:\n",
    "    \"\"\"Configuration for model trainer component.\"\"\"\n",
    "    models_dir: str\n",
    "    models: list\n",
    "    mlflow_tracking_uri: str\n",
    "    mlflow_experiment_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5980a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Trains multiple ML models and tracks experiments with MLflow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelTrainerConfig, model_params: Dict[str, Dict]):\n",
    "        \"\"\"\n",
    "        Initialize ModelTrainer component.\n",
    "        \n",
    "        Args:\n",
    "            config: ModelTrainerConfig object\n",
    "            model_params: Dictionary of model hyperparameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.model_params = model_params\n",
    "        self.models = {}\n",
    "        self.trained_models = {}\n",
    "        \n",
    "        # Setup MLflow\n",
    "        mlflow.set_tracking_uri(config.mlflow_tracking_uri)\n",
    "        mlflow.set_experiment(config.mlflow_experiment_name)\n",
    "        \n",
    "        logger.info(\"Model Trainer component initialized\")\n",
    "        logger.info(f\"MLflow tracking URI: {config.mlflow_tracking_uri}\")\n",
    "        logger.info(f\"MLflow experiment: {config.mlflow_experiment_name}\")\n",
    "    \n",
    "    def get_models(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Initialize models with their hyperparameters.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of model instances\n",
    "        \"\"\"\n",
    "        try:\n",
    "            models = {}\n",
    "            \n",
    "            if 'logistic_regression' in self.config.models:\n",
    "                models['logistic_regression'] = LogisticRegression(\n",
    "                    **self.model_params.get('logistic_regression', {})\n",
    "                )\n",
    "            \n",
    "            if 'random_forest' in self.config.models:\n",
    "                models['random_forest'] = RandomForestClassifier(\n",
    "                    **self.model_params.get('random_forest', {})\n",
    "                )\n",
    "            \n",
    "            if 'xgboost' in self.config.models:\n",
    "                models['xgboost'] = XGBClassifier(\n",
    "                    **self.model_params.get('xgboost', {})\n",
    "                )\n",
    "            \n",
    "            if 'lightgbm' in self.config.models:\n",
    "                models['lightgbm'] = LGBMClassifier(\n",
    "                    **self.model_params.get('lightgbm', {}),\n",
    "                    verbose=-1\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Initialized {len(models)} models: {list(models.keys())}\")\n",
    "            return models\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def train_model(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        model: Any, \n",
    "        X_train: np.ndarray, \n",
    "        y_train: np.ndarray,\n",
    "        X_test: np.ndarray,\n",
    "        y_test: np.ndarray\n",
    "    ) -> Tuple[Any, str]:\n",
    "        \"\"\"\n",
    "        Train a single model and log to MLflow.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model\n",
    "            model: Model instance\n",
    "            X_train: Training features\n",
    "            y_train: Training target\n",
    "            X_test: Test features\n",
    "            y_test: Test target\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (trained_model, model_path)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Training {model_name}...\")\n",
    "            \n",
    "            with mlflow.start_run(run_name=f\"{model_name}_run\") as run:\n",
    "                # Log model parameters\n",
    "                mlflow.log_params(self.model_params.get(model_name, {}))\n",
    "                \n",
    "                # Train model\n",
    "                model.fit(X_train, y_train)\n",
    "                logger.info(f\"{model_name} training completed\")\n",
    "                \n",
    "                # Make predictions\n",
    "                y_train_pred = model.predict(X_train)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                # Get prediction probabilities\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "                    y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                else:\n",
    "                    y_train_pred_proba = None\n",
    "                    y_test_pred_proba = None\n",
    "                \n",
    "                # Calculate basic metrics (detailed evaluation in model_evaluation.py)\n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "                \n",
    "                train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "                test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "                test_precision = precision_score(y_test, y_test_pred, average='binary')\n",
    "                test_recall = recall_score(y_test, y_test_pred, average='binary')\n",
    "                test_f1 = f1_score(y_test, y_test_pred, average='binary')\n",
    "                \n",
    "                # Log metrics\n",
    "                mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "                mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "                mlflow.log_metric(\"test_precision\", test_precision)\n",
    "                mlflow.log_metric(\"test_recall\", test_recall)\n",
    "                mlflow.log_metric(\"test_f1_score\", test_f1)\n",
    "                \n",
    "                logger.info(f\"{model_name} - Test Accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}\")\n",
    "                \n",
    "                # Log model to MLflow\n",
    "                mlflow.sklearn.log_model(model, f\"{model_name}_model\")\n",
    "                \n",
    "                # Save model locally\n",
    "                model_path = os.path.join(self.config.models_dir, f\"{model_name}.pkl\")\n",
    "                save_object(model_path, model)\n",
    "                logger.info(f\"{model_name} saved to: {model_path}\")\n",
    "                \n",
    "                # Log artifact path\n",
    "                mlflow.log_param(\"model_path\", model_path)\n",
    "                \n",
    "                return model, model_path\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training {model_name}\")\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def initiate_model_training(\n",
    "        self,\n",
    "        X_train: np.ndarray,\n",
    "        X_test: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        y_test: np.ndarray\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Train all configured models.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            X_test: Test features\n",
    "            y_train: Training target\n",
    "            y_test: Test target\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with trained models and their paths\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting model training process\")\n",
    "        \n",
    "        try:\n",
    "            # Get models\n",
    "            self.models = self.get_models()\n",
    "            \n",
    "            logger.info(f\"Training {len(self.models)} models\")\n",
    "            logger.info(f\"Training data shape: {X_train.shape}\")\n",
    "            logger.info(f\"Test data shape: {X_test.shape}\")\n",
    "            \n",
    "            # Train each model\n",
    "            results = {}\n",
    "            \n",
    "            for model_name, model in self.models.items():\n",
    "                logger.info(f\"\\n{'='*50}\")\n",
    "                logger.info(f\"Training {model_name}\")\n",
    "                logger.info(f\"{'='*50}\")\n",
    "                \n",
    "                trained_model, model_path = self.train_model(\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test\n",
    "                )\n",
    "                \n",
    "                results[model_name] = {\n",
    "                    'model': trained_model,\n",
    "                    'model_path': model_path\n",
    "                }\n",
    "                \n",
    "                self.trained_models[model_name] = trained_model\n",
    "            \n",
    "            logger.info(f\"\\n{'='*50}\")\n",
    "            logger.info(\"Model training completed for all models\")\n",
    "            logger.info(f\"{'='*50}\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"Error in model training\")\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bbe1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_trainer_config(config_dict: dict, mlflow_config: dict) -> ModelTrainerConfig:\n",
    "    \"\"\"\n",
    "    Create ModelTrainerConfig from dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        config_dict: Model training configuration dictionary\n",
    "        mlflow_config: MLflow configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        ModelTrainerConfig object\n",
    "    \"\"\"\n",
    "    return ModelTrainerConfig(\n",
    "        models_dir=config_dict.models_dir,\n",
    "        models=config_dict.models,\n",
    "        mlflow_tracking_uri=mlflow_config.tracking_uri,\n",
    "        mlflow_experiment_name=mlflow_config.experiment_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd050397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-07 17:58:41,705] INFO - ChurnPrediction - yaml file: configs\\model_config.yaml loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logistic_regression': ConfigBox({'C': 1.0, 'max_iter': 1000, 'random_state': 42, 'class_weight': 'balanced', 'solver': 'lbfgs', 'penalty': 'l2'})}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {}\n",
    "model_config_params = read_yaml(Path(\"configs/model_config.yaml\"))\n",
    "params[\"logistic_regression\"] = model_config_params.logistic_regression\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "40309a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-07 17:58:41,873] INFO - ChurnPrediction - yaml file: configs\\config.yaml loaded successfully\n",
      "[2025-11-07 17:58:41,877] INFO - ChurnPrediction - yaml file: configs\\model_config.yaml loaded successfully\n"
     ]
    }
   ],
   "source": [
    "config_dict = read_yaml(Path(\"configs/config.yaml\"))\n",
    "model_config_params = read_yaml(Path(\"configs/model_config.yaml\"))\n",
    "\n",
    "\n",
    "# Get model parameters\n",
    "models_params = {}\n",
    "for model_name in config_dict.model_training.models:\n",
    "    models_params[model_name] = model_config_params[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b243ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-07 18:13:53,590] INFO - ChurnPrediction - yaml file: configs\\config.yaml loaded successfully\n",
      "[2025-11-07 18:13:53,599] INFO - ChurnPrediction - yaml file: configs\\model_config.yaml loaded successfully\n",
      "[2025-11-07 18:13:53,616] INFO - ChurnPrediction - Model Trainer component initialized\n",
      "[2025-11-07 18:13:53,618] INFO - ChurnPrediction - MLflow tracking URI: mlruns\n",
      "[2025-11-07 18:13:53,619] INFO - ChurnPrediction - MLflow experiment: churn_prediction\n",
      "[2025-11-07 18:13:53,620] INFO - ChurnPrediction - Starting model training process\n",
      "[2025-11-07 18:13:53,626] INFO - ChurnPrediction - Initialized 4 models: ['logistic_regression', 'random_forest', 'xgboost', 'lightgbm']\n",
      "[2025-11-07 18:13:53,629] INFO - ChurnPrediction - Training 4 models\n",
      "[2025-11-07 18:13:53,630] INFO - ChurnPrediction - Training data shape: (5619, 30)\n",
      "[2025-11-07 18:13:53,632] INFO - ChurnPrediction - Test data shape: (1409, 30)\n",
      "[2025-11-07 18:13:53,633] INFO - ChurnPrediction - \n",
      "==================================================\n",
      "[2025-11-07 18:13:53,634] INFO - ChurnPrediction - Training logistic_regression\n",
      "[2025-11-07 18:13:53,636] INFO - ChurnPrediction - ==================================================\n",
      "[2025-11-07 18:13:53,638] INFO - ChurnPrediction - Training logistic_regression...\n",
      "[2025-11-07 18:13:53,864] INFO - ChurnPrediction - logistic_regression training completed\n",
      "[2025-11-07 18:13:53,896] INFO - ChurnPrediction - logistic_regression - Test Accuracy: 0.7388, F1: 0.6151\n",
      "c:\\Users\\44787\\anaconda3\\envs\\mlopspro\\Lib\\site-packages\\_distutils_hack\\__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\44787\\anaconda3\\envs\\mlopspro\\Lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "[2025-11-07 18:13:59,409] INFO - ChurnPrediction - logistic_regression saved to: artifacts/models\\logistic_regression.pkl\n",
      "[2025-11-07 18:13:59,420] INFO - ChurnPrediction - \n",
      "==================================================\n",
      "[2025-11-07 18:13:59,420] INFO - ChurnPrediction - Training random_forest\n",
      "[2025-11-07 18:13:59,421] INFO - ChurnPrediction - ==================================================\n",
      "[2025-11-07 18:13:59,422] INFO - ChurnPrediction - Training random_forest...\n",
      "[2025-11-07 18:13:59,740] INFO - ChurnPrediction - random_forest training completed\n",
      "[2025-11-07 18:13:59,916] INFO - ChurnPrediction - random_forest - Test Accuracy: 0.7700, F1: 0.6368\n",
      "c:\\Users\\44787\\anaconda3\\envs\\mlopspro\\Lib\\site-packages\\_distutils_hack\\__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\44787\\anaconda3\\envs\\mlopspro\\Lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "[2025-11-07 18:14:04,846] INFO - ChurnPrediction - random_forest saved to: artifacts/models\\random_forest.pkl\n",
      "[2025-11-07 18:14:04,856] INFO - ChurnPrediction - \n",
      "==================================================\n",
      "[2025-11-07 18:14:04,857] INFO - ChurnPrediction - Training xgboost\n",
      "[2025-11-07 18:14:04,857] INFO - ChurnPrediction - ==================================================\n",
      "[2025-11-07 18:14:04,858] INFO - ChurnPrediction - Training xgboost...\n",
      "[2025-11-07 18:14:05,052] INFO - ChurnPrediction - xgboost training completed\n",
      "[2025-11-07 18:14:05,103] INFO - ChurnPrediction - xgboost - Test Accuracy: 0.7928, F1: 0.5756\n",
      "c:\\Users\\44787\\anaconda3\\envs\\mlopspro\\Lib\\site-packages\\_distutils_hack\\__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\44787\\anaconda3\\envs\\mlopspro\\Lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "[2025-11-07 18:14:09,687] INFO - ChurnPrediction - xgboost saved to: artifacts/models\\xgboost.pkl\n",
      "[2025-11-07 18:14:09,699] INFO - ChurnPrediction - \n",
      "==================================================\n",
      "[2025-11-07 18:14:09,701] INFO - ChurnPrediction - Training lightgbm\n",
      "[2025-11-07 18:14:09,701] INFO - ChurnPrediction - ==================================================\n",
      "[2025-11-07 18:14:09,701] INFO - ChurnPrediction - Training lightgbm...\n",
      "[2025-11-07 18:14:09,857] INFO - ChurnPrediction - lightgbm training completed\n",
      "[2025-11-07 18:14:09,916] INFO - ChurnPrediction - lightgbm - Test Accuracy: 0.7530, F1: 0.6258\n",
      "c:\\Users\\44787\\anaconda3\\envs\\mlopspro\\Lib\\site-packages\\_distutils_hack\\__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\44787\\anaconda3\\envs\\mlopspro\\Lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "[2025-11-07 18:14:14,991] INFO - ChurnPrediction - lightgbm saved to: artifacts/models\\lightgbm.pkl\n",
      "[2025-11-07 18:14:15,003] INFO - ChurnPrediction - \n",
      "==================================================\n",
      "[2025-11-07 18:14:15,004] INFO - ChurnPrediction - Model training completed for all models\n",
      "[2025-11-07 18:14:15,006] INFO - ChurnPrediction - ==================================================\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_dict = read_yaml(Path(\"configs/config.yaml\"))\n",
    "    model_config_params = read_yaml(Path(\"configs/model_config.yaml\"))\n",
    "    \n",
    "    model_training_config = create_model_trainer_config(\n",
    "        config_dict.model_training,\n",
    "        config_dict.mlflow)\n",
    "    \n",
    "    # Get model parameters\n",
    "    model_params = {}\n",
    "    for model_name in config_dict.model_training.models:\n",
    "        model_params[model_name] = model_config_params[model_name]\n",
    "    trained_models = ModelTrainer(\n",
    "        config=model_training_config,\n",
    "        model_params=model_params\n",
    "    ).initiate_model_training(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Model training failed: {e}\")\n",
    "    raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721e6a4",
   "metadata": {},
   "source": [
    "<h1 align=center>Model Evaluation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4029f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "# from src.utils.common import save_json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e747920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(path: str, data: dict):\n",
    "    \"\"\"\n",
    "    Save a dictionary as a JSON file.\n",
    "    \"\"\"\n",
    "    # ensure directory exists\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3a7ac740",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "    \"\"\"Configuration for model evaluation component.\"\"\"\n",
    "    metrics_path: str\n",
    "    threshold: float = 0.5\n",
    "    min_f1_score: float = 0.75\n",
    "    min_roc_auc: float = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5cbdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    \"\"\"\n",
    "    Evaluates trained models and generates comprehensive metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        \"\"\"\n",
    "        Initialize ModelEvaluation component.\n",
    "        \n",
    "        Args:\n",
    "            config: ModelEvaluationConfig object\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        logger.info(\"Model Evaluation component initialized\")\n",
    "    \n",
    "    def calculate_metrics(\n",
    "        self,\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        y_pred_proba: np.ndarray = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Calculate comprehensive evaluation metrics.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels\n",
    "            y_pred: Predicted labels\n",
    "            y_pred_proba: Prediction probabilities (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            metrics = {}\n",
    "            \n",
    "            # Classification metrics\n",
    "            metrics['accuracy'] = float(accuracy_score(y_true, y_pred))\n",
    "            metrics['precision'] = float(precision_score(y_true, y_pred, average='binary'))\n",
    "            metrics['recall'] = float(recall_score(y_true, y_pred, average='binary'))\n",
    "            metrics['f1_score'] = float(f1_score(y_true, y_pred, average='binary'))\n",
    "            \n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            metrics['confusion_matrix'] = {\n",
    "                'tn': int(cm[0, 0]),\n",
    "                'fp': int(cm[0, 1]),\n",
    "                'fn': int(cm[1, 0]),\n",
    "                'tp': int(cm[1, 1])\n",
    "            }\n",
    "            \n",
    "            # Derived metrics from confusion matrix\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            metrics['specificity'] = float(tn / (tn + fp)) if (tn + fp) > 0 else 0.0\n",
    "            metrics['sensitivity'] = float(tp / (tp + fn)) if (tp + fn) > 0 else 0.0\n",
    "            \n",
    "            # ROC AUC if probabilities are available\n",
    "            if y_pred_proba is not None:\n",
    "                metrics['roc_auc'] = float(roc_auc_score(y_true, y_pred_proba))\n",
    "            \n",
    "            # Classification report\n",
    "            report = classification_report(y_true, y_pred, output_dict=True)\n",
    "            metrics['classification_report'] = report\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        model: Any,\n",
    "        X_train: np.ndarray,\n",
    "        X_test: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        y_test: np.ndarray\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate a single model on train and test data.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model\n",
    "            model: Trained model instance\n",
    "            X_train: Training features\n",
    "            X_test: Test features\n",
    "            y_train: Training target\n",
    "            y_test: Test target\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Evaluating {model_name}...\")\n",
    "            \n",
    "            # Predictions\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "            \n",
    "            # Prediction probabilities\n",
    "            y_train_pred_proba = None\n",
    "            y_test_pred_proba = None\n",
    "            \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "                y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_metrics = self.calculate_metrics(y_train, y_train_pred, y_train_pred_proba)\n",
    "            test_metrics = self.calculate_metrics(y_test, y_test_pred, y_test_pred_proba)\n",
    "            \n",
    "            evaluation_results = {\n",
    "                'model_name': model_name,\n",
    "                'train_metrics': train_metrics,\n",
    "                'test_metrics': test_metrics,\n",
    "                'threshold': self.config.threshold\n",
    "            }\n",
    "            \n",
    "            # Check if model meets minimum requirements\n",
    "            meets_requirements = (\n",
    "                test_metrics['f1_score'] >= self.config.min_f1_score and\n",
    "                test_metrics.get('roc_auc', 0) >= self.config.min_roc_auc\n",
    "            )\n",
    "            \n",
    "            evaluation_results['meets_requirements'] = meets_requirements\n",
    "            \n",
    "            # Log summary\n",
    "            logger.info(f\"\\n{model_name} Evaluation Results:\")\n",
    "            logger.info(f\"  Train Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "            logger.info(f\"  Test Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "            logger.info(f\"  Test Precision: {test_metrics['precision']:.4f}\")\n",
    "            logger.info(f\"  Test Recall:    {test_metrics['recall']:.4f}\")\n",
    "            logger.info(f\"  Test F1-Score:  {test_metrics['f1_score']:.4f}\")\n",
    "            if 'roc_auc' in test_metrics:\n",
    "                logger.info(f\"  Test ROC-AUC:   {test_metrics['roc_auc']:.4f}\")\n",
    "            logger.info(f\"  Meets Requirements: {meets_requirements}\")\n",
    "            \n",
    "            return evaluation_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating {model_name}\")\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def compare_models(self, evaluation_results: Dict[str, Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compare all evaluated models and select the best one.\n",
    "        \n",
    "        Args:\n",
    "            evaluation_results: Dictionary of evaluation results for all models\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with comparison results and best model info\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"\\nComparing models...\")\n",
    "            \n",
    "            comparison = []\n",
    "            \n",
    "            for model_name, results in evaluation_results.items():\n",
    "                test_metrics = results['test_metrics']\n",
    "                \n",
    "                comparison.append({\n",
    "                    'model_name': model_name,\n",
    "                    'accuracy': test_metrics['accuracy'],\n",
    "                    'precision': test_metrics['precision'],\n",
    "                    'recall': test_metrics['recall'],\n",
    "                    'f1_score': test_metrics['f1_score'],\n",
    "                    'roc_auc': test_metrics.get('roc_auc', 0),\n",
    "                    'meets_requirements': results['meets_requirements']\n",
    "                })\n",
    "            \n",
    "            # Create comparison DataFrame\n",
    "            comparison_df = pd.DataFrame(comparison)\n",
    "            comparison_df = comparison_df.sort_values('f1_score', ascending=False)\n",
    "            \n",
    "            # Select best model based on F1 score\n",
    "            best_model = comparison_df.iloc[0].to_dict()\n",
    "            \n",
    "            logger.info(\"\\nModel Comparison (sorted by F1-Score):\")\n",
    "            logger.info(\"\\n\" + comparison_df.to_string(index=False))\n",
    "            logger.info(f\"\\nBest Model: {best_model['model_name']}\")\n",
    "            logger.info(f\"  F1-Score: {best_model['f1_score']:.4f}\")\n",
    "            logger.info(f\"  ROC-AUC:  {best_model['roc_auc']:.4f}\")\n",
    "            \n",
    "            return {\n",
    "                'comparison_table': comparison_df.to_dict('records'),\n",
    "                'best_model': best_model\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def initiate_model_evaluation(\n",
    "        self,\n",
    "        trained_models: Dict[str, Any],\n",
    "        X_train: np.ndarray,\n",
    "        X_test: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        y_test: np.ndarray\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate all trained models and compare them.\n",
    "        \n",
    "        Args:\n",
    "            trained_models: Dictionary of trained models\n",
    "            X_train: Training features\n",
    "            X_test: Test features\n",
    "            y_train: Training target\n",
    "            y_test: Test target\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with all evaluation results\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting model evaluation process\")\n",
    "        \n",
    "        try:\n",
    "            evaluation_results = {}\n",
    "            \n",
    "            # Evaluate each model\n",
    "            for model_name, model_info in trained_models.items():\n",
    "                model = model_info['model']\n",
    "                \n",
    "                results = self.evaluate_model(\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    X_train=X_train,\n",
    "                    X_test=X_test,\n",
    "                    y_train=y_train,\n",
    "                    y_test=y_test\n",
    "                )\n",
    "                \n",
    "                evaluation_results[model_name] = results\n",
    "            \n",
    "            # Compare models\n",
    "            comparison_results = self.compare_models(evaluation_results)\n",
    "            \n",
    "            # Compile final report\n",
    "            final_report = {\n",
    "                'individual_evaluations': evaluation_results,\n",
    "                'comparison': comparison_results,\n",
    "                'best_model_name': comparison_results['best_model']['model_name']\n",
    "            }\n",
    "            \n",
    "            # Save evaluation report\n",
    "            os.makedirs(self.config.metrics_path, exist_ok=True)\n",
    "            report_path = os.path.join(self.config.metrics_path, 'evaluation_report.json')\n",
    "            save_json(report_path, final_report)\n",
    "            \n",
    "            logger.info(f\"\\nEvaluation report saved to: {report_path}\")\n",
    "            logger.info(\"Model evaluation completed successfully\")\n",
    "            \n",
    "            return final_report\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(\"Error in model evaluation\")\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dade84fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_evaluation_config(config_dict: dict) -> ModelEvaluationConfig:\n",
    "    \"\"\"\n",
    "    Create ModelEvaluationConfig from dictionary.\n",
    "    \n",
    "    Args:\n",
    "        config_dict: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        ModelEvaluationConfig object\n",
    "    \"\"\"\n",
    "    return ModelEvaluationConfig(\n",
    "        metrics_path=config_dict.metrics_path,\n",
    "        threshold=config_dict.threshold,\n",
    "        min_f1_score=config_dict.min_f1_score,\n",
    "        min_roc_auc=config_dict.min_roc_auc\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "65f36ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
       " 'model_path': 'artifacts/models\\\\logistic_regression.pkl'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models['logistic_regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a9c6bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-07 18:29:02,911] INFO - ChurnPrediction - yaml file: configs\\config.yaml loaded successfully\n",
      "[2025-11-07 18:29:02,913] INFO - ChurnPrediction - Model Evaluation component initialized\n",
      "[2025-11-07 18:29:02,914] INFO - ChurnPrediction - Starting model evaluation process\n",
      "[2025-11-07 18:29:02,916] INFO - ChurnPrediction - Evaluating logistic_regression...\n",
      "[2025-11-07 18:29:02,976] INFO - ChurnPrediction - \n",
      "logistic_regression Evaluation Results:\n",
      "[2025-11-07 18:29:02,976] INFO - ChurnPrediction -   Train Accuracy: 0.7533\n",
      "[2025-11-07 18:29:02,982] INFO - ChurnPrediction -   Test Accuracy:  0.7388\n",
      "[2025-11-07 18:29:02,982] INFO - ChurnPrediction -   Test Precision: 0.5052\n",
      "[2025-11-07 18:29:02,982] INFO - ChurnPrediction -   Test Recall:    0.7861\n",
      "[2025-11-07 18:29:02,982] INFO - ChurnPrediction -   Test F1-Score:  0.6151\n",
      "[2025-11-07 18:29:02,982] INFO - ChurnPrediction -   Test ROC-AUC:   0.8416\n",
      "[2025-11-07 18:29:02,982] INFO - ChurnPrediction -   Meets Requirements: False\n",
      "[2025-11-07 18:29:02,982] INFO - ChurnPrediction - Evaluating random_forest...\n",
      "[2025-11-07 18:29:03,167] INFO - ChurnPrediction - \n",
      "random_forest Evaluation Results:\n",
      "[2025-11-07 18:29:03,168] INFO - ChurnPrediction -   Train Accuracy: 0.8388\n",
      "[2025-11-07 18:29:03,170] INFO - ChurnPrediction -   Test Accuracy:  0.7700\n",
      "[2025-11-07 18:29:03,171] INFO - ChurnPrediction -   Test Precision: 0.5483\n",
      "[2025-11-07 18:29:03,171] INFO - ChurnPrediction -   Test Recall:    0.7594\n",
      "[2025-11-07 18:29:03,171] INFO - ChurnPrediction -   Test F1-Score:  0.6368\n",
      "[2025-11-07 18:29:03,171] INFO - ChurnPrediction -   Test ROC-AUC:   0.8417\n",
      "[2025-11-07 18:29:03,171] INFO - ChurnPrediction -   Meets Requirements: False\n",
      "[2025-11-07 18:29:03,171] INFO - ChurnPrediction - Evaluating xgboost...\n",
      "[2025-11-07 18:29:03,237] INFO - ChurnPrediction - \n",
      "xgboost Evaluation Results:\n",
      "[2025-11-07 18:29:03,238] INFO - ChurnPrediction -   Train Accuracy: 0.8717\n",
      "[2025-11-07 18:29:03,239] INFO - ChurnPrediction -   Test Accuracy:  0.7928\n",
      "[2025-11-07 18:29:03,240] INFO - ChurnPrediction -   Test Precision: 0.6306\n",
      "[2025-11-07 18:29:03,241] INFO - ChurnPrediction -   Test Recall:    0.5294\n",
      "[2025-11-07 18:29:03,241] INFO - ChurnPrediction -   Test F1-Score:  0.5756\n",
      "[2025-11-07 18:29:03,242] INFO - ChurnPrediction -   Test ROC-AUC:   0.8350\n",
      "[2025-11-07 18:29:03,243] INFO - ChurnPrediction -   Meets Requirements: False\n",
      "[2025-11-07 18:29:03,243] INFO - ChurnPrediction - Evaluating lightgbm...\n",
      "[2025-11-07 18:29:03,301] INFO - ChurnPrediction - \n",
      "lightgbm Evaluation Results:\n",
      "[2025-11-07 18:29:03,302] INFO - ChurnPrediction -   Train Accuracy: 0.8284\n",
      "[2025-11-07 18:29:03,303] INFO - ChurnPrediction -   Test Accuracy:  0.7530\n",
      "[2025-11-07 18:29:03,304] INFO - ChurnPrediction -   Test Precision: 0.5234\n",
      "[2025-11-07 18:29:03,306] INFO - ChurnPrediction -   Test Recall:    0.7781\n",
      "[2025-11-07 18:29:03,307] INFO - ChurnPrediction -   Test F1-Score:  0.6258\n",
      "[2025-11-07 18:29:03,307] INFO - ChurnPrediction -   Test ROC-AUC:   0.8349\n",
      "[2025-11-07 18:29:03,308] INFO - ChurnPrediction -   Meets Requirements: False\n",
      "[2025-11-07 18:29:03,309] INFO - ChurnPrediction - \n",
      "Comparing models...\n",
      "[2025-11-07 18:29:03,355] INFO - ChurnPrediction - \n",
      "Model Comparison (sorted by F1-Score):\n",
      "[2025-11-07 18:29:03,364] INFO - ChurnPrediction - \n",
      "         model_name  accuracy  precision   recall  f1_score  roc_auc  meets_requirements\n",
      "      random_forest  0.770050   0.548263 0.759358  0.636771 0.841732               False\n",
      "           lightgbm  0.753016   0.523381 0.778075  0.625806 0.834922               False\n",
      "logistic_regression  0.738822   0.505155 0.786096  0.615063 0.841579               False\n",
      "            xgboost  0.792761   0.630573 0.529412  0.575581 0.835032               False\n",
      "[2025-11-07 18:29:03,365] INFO - ChurnPrediction - \n",
      "Best Model: random_forest\n",
      "[2025-11-07 18:29:03,366] INFO - ChurnPrediction -   F1-Score: 0.6368\n",
      "[2025-11-07 18:29:03,368] INFO - ChurnPrediction -   ROC-AUC:  0.8417\n",
      "[2025-11-07 18:29:03,375] INFO - ChurnPrediction - \n",
      "Evaluation report saved to: artifacts/metrics\\evaluation_report.json\n",
      "[2025-11-07 18:29:03,376] INFO - ChurnPrediction - Model evaluation completed successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_dict = read_yaml(Path(\"configs/config.yaml\"))\n",
    "    config = create_model_evaluation_config(config_dict.model_evaluation)\n",
    "    evaluation_report = ModelEvaluation(config).initiate_model_evaluation(\n",
    "        trained_models=trained_models,\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Model evaluation failed: {e}\")\n",
    "    raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b80b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlopspro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
